{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5743a764-d92c-478f-a85b-6641fcf6f922",
   "metadata": {},
   "source": [
    "## General function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74703a-cdb6-4c7d-a71e-ba2b88ff06b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get satellite Imegary from Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04014523-4ed9-4459-913a-112ca7667293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Apr 8 14:13:00 2023\n",
    "    By Saeed Rahimi\n",
    "    V 1.0 Date 08/04/2023\n",
    "    This madual includes several functions for downlaoding the satelite imageries from Google Earth Engine:\n",
    "        Search for the imagery during the specified time-period\n",
    "        Detects and removes the clouds in the scene\n",
    "        Takes the median value of each pixel in all images in the stack (for the specified time-period)\n",
    "        Creates a composite image\n",
    "    @author: sarsh\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import ee\n",
    "    print('The required modules are installed')\n",
    "except ModuleNotFoundError:\n",
    "    print('The required modules are NOT installed')\n",
    "\n",
    "    # Install the required modules\n",
    "    python = sys.executable\n",
    "    subprocess.check_call(\n",
    "        [python, '-m', 'pip', 'install', 'earthengine-api'],\n",
    "        stdout=subprocess.DEVNULL\n",
    "    )\n",
    "\n",
    "## Initialize the library.\n",
    "ee.Initialize()\n",
    "\n",
    "# Configure the cloud details\n",
    "CONFIGURATION = {'CLOUD_FILTER': 80, 'CLD_PRB_THRESH': 50, 'NIR_DRK_THRESH': 0.15,\n",
    "                 'CLD_PRJ_DIST': 1, 'BUFFER': 50}\n",
    "\n",
    "# Get a cloud free Sentinal2 composite from a colection between the START_DATE and END_DATE\n",
    "def cld_free_sl2(START_DATE, END_DATE, AOI):\n",
    "## %% Build a Sentinel-2 collection\n",
    "    ## Define build a Sentinel-2 collection function\n",
    "    def get_s2_sr_cld_col(start_date, end_date, aoi):\n",
    "        ## Import and filter S2 SR.\n",
    "        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "            .filterBounds(aoi)\n",
    "            ## .filter(ee.Filter.calendarRange(START_YEAR,END_YEAR,'year'))\n",
    "            ## .filter(ee.Filter.calendarRange(START_MONTH,END_MONTH,'month'))\n",
    "            .filterDate(start_date, end_date)\n",
    "            .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "        ## Import and filter s2cloudless.\n",
    "        s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "            .filterBounds(aoi)\n",
    "            ## .filter(ee.Filter.calendarRange(START_YEAR,END_YEAR,'year'))\n",
    "            ## .filter(ee.Filter.calendarRange(START_MONTH,END_MONTH,'month'))\n",
    "            .filterDate(start_date, end_date))\n",
    "\n",
    "\n",
    "        ## Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "        return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "            'primary': s2_sr_col,\n",
    "            'secondary': s2_cloudless_col,\n",
    "            'condition': ee.Filter.equals(**{\n",
    "                'leftField': 'system:index',\n",
    "                'rightField': 'system:index'\n",
    "            })\n",
    "        }))\n",
    "\n",
    "## %% Cloud components\n",
    "    def add_cloud_bands(img):\n",
    "        ## Get s2cloudless image, subset the probability band.\n",
    "        cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "        ## Condition s2cloudless by the probability threshold value.\n",
    "        is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "        ## Add the cloud probability layer and cloud mask as image bands.\n",
    "        return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "\n",
    "\n",
    "## %% Cloud shadow components\n",
    "    def add_shadow_bands(img):\n",
    "        ## Identify water pixels from the SCL band.\n",
    "        not_water = img.select('SCL').neq(6)\n",
    "\n",
    "        ## Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "        SR_BAND_SCALE = 1e4\n",
    "        dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "        ## Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "        shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "        ## Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "        cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "            .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "            .select('distance')\n",
    "            .mask()\n",
    "            .rename('cloud_transform'))\n",
    "\n",
    "        ## Identify the intersection of dark pixels with cloud shadow projection.\n",
    "        shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "        ## Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "        return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "\n",
    "\n",
    "    ## %% Final cloud-shadow mask\n",
    "    def add_cld_shdw_mask(img):\n",
    "        ## Add cloud component bands.\n",
    "        img_cloud = add_cloud_bands(img)\n",
    "\n",
    "        ## Add cloud shadow component bands.\n",
    "        img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "        ## Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "        is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "        ## Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "        ## 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "        is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "            .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "            .rename('cloudmask'))\n",
    "\n",
    "        ## Add the final cloud-shadow mask to the image.\n",
    "        return img.addBands(is_cld_shdw)\n",
    "\n",
    "    ## %% Define cloud mask application function\n",
    "    def apply_cld_shdw_mask(img):\n",
    "        ## Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "        not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "        ## Subset reflectance bands and update their masks, return the result.\n",
    "        return img.select('B.*').updateMask(not_cld_shdw)\n",
    "\n",
    "    ## Configure the cloud details\n",
    "    CLOUD_FILTER = CONFIGURATION['CLOUD_FILTER']\n",
    "    CLD_PRB_THRESH = CONFIGURATION['CLD_PRB_THRESH']\n",
    "    NIR_DRK_THRESH = CONFIGURATION['NIR_DRK_THRESH']\n",
    "    CLD_PRJ_DIST = CONFIGURATION['CLD_PRJ_DIST']\n",
    "    BUFFER = CONFIGURATION['BUFFER']\n",
    "\n",
    "    ## %% Return the cloud free single image\n",
    "    s2_sr_cld_col = get_s2_sr_cld_col(START_DATE, END_DATE, AOI)\n",
    "    s2_sr_cldless = (s2_sr_cld_col.filterBounds(AOI)\n",
    "                                  .map(add_cld_shdw_mask)\n",
    "                                  .map(apply_cld_shdw_mask)\n",
    "                                  .median())\n",
    "    return s2_sr_cldless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cbacb-10b5-4c82-87ce-312a41ff4573",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Calculate a list of indices from a satellite image (an ee.image() object) and stack the results as an ee.image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04328a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Function to calculate a list of indices for a specific site and return the image\n",
    "def calculate_indices(img, indices, ee_polygon):        \n",
    "    # Dictionary mapping index name to calculation function\n",
    "    index_functions = {\n",
    "        'MNDWI': lambda img: img.normalizedDifference(['B3', 'B11']).rename(\"MNDWI\"),\n",
    "        'NDVI': lambda img: img.normalizedDifference(['B8', 'B4']).rename(\"NDVI\"),\n",
    "        'NIRv': lambda img: img.normalizedDifference(['B8', 'B4']).multiply(img.select('B8')).rename(\"NIRv\"),\n",
    "        'NSMI': lambda img: img.normalizedDifference(['B8', 'B11']).rename(\"NSMI\"),\n",
    "        'BSI': lambda img: img.normalizedDifference(['B3', 'B11']).subtract(img.select('B8')).divide(img.select('B11').add(img.select('B8'))).rename(\"BSI\"),\n",
    "        'EVI': lambda img: img.expression('2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', \n",
    "                                          {'NIR': img.select('B8'), 'RED': img.select('B4'), 'BLUE': img.select('B2')}).rename(\"EVI\"),\n",
    "        'SAVI': lambda img: img.expression('(1 + 0.5) * (NIR - RED) / (NIR + RED + 0.5)',\n",
    "                                            {'NIR': img.select('B8'), 'RED': img.select('B4')}).rename(\"SAVI\"),\n",
    "        'NDMI': lambda img: img.normalizedDifference(['B8', 'B11']).rename(\"NDMI\"),\n",
    "        'NBR': lambda img: img.normalizedDifference(['B8', 'B12']).rename(\"NBR\"),\n",
    "        'CI': lambda img: img.select('B4').divide(img.select('B3')).subtract(1).rename(\"CI\"),\n",
    "        'LAI': lambda img: img.expression('3.618 * exp(-0.488 * (RED - GREEN))',\n",
    "                                           {'RED': img.select('B4'), 'GREEN': img.select('B3')}).rename(\"LAI\"),\n",
    "        'FAPAR': lambda img: img.expression('(NIR - RED) / (0.88 * NIR + 0.12 * RED)',\n",
    "                                             {'NIR': img.select('B8'), 'RED': img.select('B4')}).rename(\"FAPAR\"),\n",
    "    }\n",
    "\n",
    "    # Initialize an empty ee.Image to store the calculated indices\n",
    "    calc_indices = ee.Image()\n",
    "\n",
    "    # Iterate over the list of indices and calculate each requested index in the list\n",
    "    for index in indices:\n",
    "        if index in index_functions:\n",
    "            # Call the corresponding function to calculate the index\n",
    "            index_calculation = index_functions[index](img)\n",
    "            calc_indices = calc_indices.addBands(index_calculation)\n",
    "\n",
    "    # Clip the index image to the extent of the current sampling site catchment\n",
    "    calc_indices = calc_indices.clip(ee_polygon)\n",
    "\n",
    "    return calc_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74addf6d-26ed-48be-9c39-d044eb4365c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convert polygons in a shapefile to ee.FeatureCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baf12d-62d7-469b-8e55-01dc31da1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import ee\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import geopandas as gpd\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# convert a shapefile to an ee_collection\n",
    "def shapefile_2_ee_collection(gdf):\n",
    "    # Transform the geometry to WGS84 projection\n",
    "    source_crs = \"EPSG:2193\"\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "   \n",
    "    def geometry_to_ee_polygon(geometry): \n",
    "        # Reproject the polygon to the target CRS\n",
    "        try:\n",
    "            # Check if the geometery is a polygon or a multipolygon type\n",
    "            if isinstance(geometry, MultiPolygon):\n",
    "                geometry_list = []\n",
    "                for polygon in geometry.geoms:\n",
    "                    # Reproject the polygon to the target CRS\n",
    "                    transformed_coordinates = [transformer.transform(x, y) for x, y in polygon.exterior.coords]\n",
    "                    transformed_geometry = Polygon(transformed_coordinates)\n",
    "                    \n",
    "                    # Create an ee.Geometry.Polygon and add it to a list\n",
    "                    ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "                    geometry_list += [ee_polygon]\n",
    "                    \n",
    "                    # Create a ee.Geometry.MultiPolygon using the list of ee.Geometry objects\n",
    "                    multi_polygon = ee.Geometry.MultiPolygon(geometry_list)\n",
    "                    # Dissolve the ee.Geometry.MultiPolygon to create a single ee.Geometry.Polygon\n",
    "                    ee_polygon = multi_polygon.dissolve()\n",
    "            \n",
    "            elif isinstance(geometry, Polygon):\n",
    "                # Reproject the polygon to the target CRS\n",
    "                transformed_coordinates = [transformer.transform(x, y) for x, y in geometry.exterior.coords]\n",
    "                transformed_geometry = Polygon(transformed_coordinates)\n",
    "                ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid geometry type. Must be Polygon or MultiPolygon.\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Invalid geometry at index: {e}\")\n",
    "        \n",
    "        return ee_polygon\n",
    "\n",
    "    # Create a list of ee.Geometry objects\n",
    "    polygon_list = [geometry_to_ee_polygon(geometry) for geometry in gdf['geometry']]\n",
    "    \n",
    "    # Convert the list to an ee.FeatureCollection\n",
    "    polygon_collection = ee.FeatureCollection(polygon_list)\n",
    "\n",
    "    return polygon_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb4db4-665c-4bc4-affe-6a93b0cda3df",
   "metadata": {},
   "source": [
    "## Iterate over each capture zone (sampling point) and collect the RS indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8aee0-77f1-4aa4-80cf-61b2822724a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Iterate over the sampling sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701438ca-261b-4ef6-88de-1b2e4ebcca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ee\n",
    "\n",
    "# # Initialize Earth Engine\n",
    "# ee.Initialize()\n",
    "\n",
    "# # Function to perform zonal stats\n",
    "# def calculate_stats(image, indices, ee_polygon):    \n",
    "#     # Create an empty dictionary\n",
    "#     statistics_dict = {}\n",
    "    \n",
    "#     # Calculate statistics for each index\n",
    "#     for index in indices:\n",
    "#         # Calculate multiple statistics using reduceRegion\n",
    "#         stats = image.reduceRegion(\n",
    "#             reducer=ee.Reducer.min().combine(\n",
    "#                 reducer2=ee.Reducer.median().combine(\n",
    "#                     reducer2=ee.Reducer.max().combine(\n",
    "#                         reducer2=ee.Reducer.stdDev(),\n",
    "#                         sharedInputs=True\n",
    "#                     ),\n",
    "#                     sharedInputs=True\n",
    "#                 ),\n",
    "#                 sharedInputs=True\n",
    "#             ),\n",
    "#             geometry=ee_polygon,\n",
    "#             scale=10,\n",
    "#             maxPixels=1e19\n",
    "#         )\n",
    "        \n",
    "#         # Add the statistics to the dictionary\n",
    "#         statistics_dict[index] = {\n",
    "#           # 'min': stats.get(f'{index}_min').getInfo(),\n",
    "#           'median': stats.get(f'{index}_median').getInfo(),\n",
    "#           # 'max': stats.get(f'{index}_max').getInfo(),\n",
    "#           'stdv': stats.get(f'{index}_stdDev').getInfo()\n",
    "#         }\n",
    "        \n",
    "#     return statistics_dict\n",
    "\n",
    "# # Function to convert the geometry to an ee.polygon and transform the projection\n",
    "# def geometry_to_ee_polygon(geometry):\n",
    "#         # Transform the geometry to WGS84 projection\n",
    "#         source_crs = \"EPSG:2193\"\n",
    "#         target_crs = \"EPSG:4326\"\n",
    "#         transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "#         geometry = site['geometry']\n",
    "    \n",
    "#         # Reproject the polygon to the target CRS\n",
    "#         try:\n",
    "#             # Check if the geometery is a polygon or a multipolygon type\n",
    "#             if isinstance(geometry, MultiPolygon):\n",
    "#                 geometry_list = []\n",
    "#                 for polygon in geometry.geoms:\n",
    "#                     # Reproject the polygon to the target CRS\n",
    "#                     transformed_coordinates = [transformer.transform(x, y) for x, y in polygon.exterior.coords]\n",
    "#                     transformed_geometry = Polygon(transformed_coordinates)\n",
    "                    \n",
    "#                     # Create an ee.Geometry.Polygon and add it to a list\n",
    "#                     ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "#                     geometry_list += [ee_polygon]\n",
    "                    \n",
    "#                     # Create a ee.Geometry.MultiPolygon using the list of ee.Geometry objects\n",
    "#                     multi_polygon = ee.Geometry.MultiPolygon(geometry_list)\n",
    "#                     # Dissolve the ee.Geometry.MultiPolygon to create a single ee.Geometry.Polygon\n",
    "#                     ee_polygon = multi_polygon.dissolve()\n",
    "            \n",
    "#             elif isinstance(geometry, Polygon):\n",
    "#                 # Reproject the polygon to the target CRS\n",
    "#                 transformed_coordinates = [transformer.transform(x, y) for x, y in geometry.exterior.coords]\n",
    "#                 transformed_geometry = Polygon(transformed_coordinates)\n",
    "#                 ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "        \n",
    "#             else:\n",
    "#                 raise ValueError(\"Invalid geometry type. Must be Polygon or MultiPolygon.\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Invalid geometry at index: {e}\")\n",
    "        \n",
    "#         return ee_polygon\n",
    "\n",
    "# # Function to process each sampling site and its dates\n",
    "# def process_sampling_site(site):\n",
    "#     # Get the sampling site id\n",
    "#     seg_id = site['nzsegment']\n",
    "\n",
    "#     # Convert the geometry to an ee.polygon\n",
    "#     ee_polygon = geometry_to_ee_polygon(site['geometry'])\n",
    "\n",
    "#     # Define list of the calculated indices\n",
    "#     indices = ['MNDWI', 'NDVI', 'NIRv', 'NSMI', 'BSI', 'EVI', 'SAVI', 'NDMI', 'NBR', 'CI', 'LAI', 'FAPAR']\n",
    "\n",
    "#     # Get the sampling dates for the current sampling site\n",
    "#     sampeling_dates = filtered_stack_frame['myDate'][filtered_stack_frame['nzsegment'] == seg_id].unique()\n",
    "    \n",
    "#     dates_dict = {}\n",
    "#     fixed_date = datetime.strptime(\"2020-12-03\", \"%Y-%m-%d\").date()\n",
    "#     for date in sampeling_dates:\n",
    "#         if date > fixed_date:\n",
    "#             try:\n",
    "#                 START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "#                 END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "#                 print('.....Date in progress:', START_DATE, END_DATE)\n",
    "                \n",
    "#                 # Create a cloud free sentinel-2 image for a specific region and time\n",
    "#                 cld_free_image = cld_free_sl2(START_DATE, END_DATE, ee_polygon)\n",
    "                \n",
    "#                 # Calculated all RS indices and stack them into an EE Image.\n",
    "#                 indices_image = calculate_indices(cld_free_image, indices, ee_polygon)\n",
    "\n",
    "#                 # Calculate zonal stats\n",
    "#                 stats_date_indices_dic = calculate_stats(indices_image, indices, ee_polygon)\n",
    "       \n",
    "#                 # Define the output file path\n",
    "#                 stats_date_indices_dir = f'../Data/RS/RS_Indices_{seg_id}_{END_DATE}.p'\n",
    "                \n",
    "#                 # Save the dictionary as a pickle file\n",
    "#                 with open(stats_date_indices_dir, 'wb') as f:\n",
    "#                     pickle.dump(stats_date_indices_dic, f)\n",
    "\n",
    "#                 # Add the stats result to the dictonary\n",
    "#                 dates_dict[END_DATE]  = stats_date_indices_dic\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Something went wrong: {e}\")\n",
    "#                 continue\n",
    "        \n",
    "#     return dates_dict\n",
    "\n",
    "# import os\n",
    "# import geopandas as gpd\n",
    "# from datetime import date, timedelta, datetime\n",
    "# import pyproj\n",
    "# from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "# try:\n",
    "#     import cPickle as pickle\n",
    "# except ImportError:  # Python 3.x\n",
    "#     import pickle\n",
    "\n",
    "# # Read the pickle data\n",
    "# in_f_dir = '../Data/WQ_Data_Nov2021.p'\n",
    "# with open(in_f_dir, 'rb') as f:\n",
    "#     dat_dic = pickle.load(f)\n",
    "\n",
    "# # Prepare the sampling dataset\n",
    "# stack_frame = dat_dic['StackFrame']\n",
    "# metadata = dat_dic['metadataWQ'].drop_duplicates()\n",
    "\n",
    "# # Filter the sampling dataset to contain only 2019 and later\n",
    "# filtered_stack_frame = stack_frame[stack_frame['Year'] > 2018]\n",
    "# filtered_stack_frame = filtered_stack_frame.reset_index(drop=True)\n",
    "# filtered_metadata = metadata[metadata['nzsegment'].isin(filtered_stack_frame['nzsegment'])]\n",
    "# filtered_metadata = filtered_metadata.reset_index(drop=True)\n",
    "\n",
    "# # Initialize the empty dictionary for RS Indices\n",
    "# rs_indices = {}\n",
    "\n",
    "# # Process each sampling site using the function\n",
    "# for i, site in filtered_metadata.iterrows():\n",
    "#     # Get the sampling site id\n",
    "#     seg_id = site['nzsegment']\n",
    "#     print(f\"{i}-\", \"We are collecting data for sampling site\", seg_id)\n",
    "    \n",
    "#     # Get the sampling dates for the current sampling site\n",
    "#     sampeling_dates = filtered_stack_frame['myDate'][filtered_stack_frame['nzsegment'] == seg_id].unique()\n",
    "    \n",
    "#     if i == 101:\n",
    "#         print(i)            \n",
    "#         rs_indices[site['nzsegment']] = process_sampling_site(site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec1868-92e7-45da-a3ea-3bd6eabb3113",
   "metadata": {},
   "source": [
    "### Iterate over the observation dates in each sampling site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b285b3f-f24b-477e-96aa-e80cbd412c6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### The normal process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f33366-1e73-42cf-8914-a27611ed08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pyproj\n",
    "\n",
    "try:\n",
    "    import ee\n",
    "    print('The required modules are installed')\n",
    "except ModuleNotFoundError:\n",
    "    print('The required modules are NOT installed')\n",
    "\n",
    "    # Install the required modules\n",
    "    python = sys.executable\n",
    "    subprocess.check_call(\n",
    "        [python, '-m', 'pip', 'install', 'earthengine-api'],\n",
    "        stdout=subprocess.DEVNULL\n",
    "    )\n",
    "\n",
    "## Initialize the library.\n",
    "ee.Initialize()\n",
    "\n",
    "# Function to perform zonal stats\n",
    "def calculate_stats(image, indices, ee_polygon):    \n",
    "    # Create an empty dictionary\n",
    "    statistics_dict = {}\n",
    "    \n",
    "    # Calculate statistics for each index\n",
    "    for index in indices:\n",
    "        # Calculate multiple statistics using reduceRegion\n",
    "        stats = image.reduceRegion(\n",
    "            reducer=ee.Reducer.min().combine(\n",
    "                reducer2=ee.Reducer.median().combine(\n",
    "                    reducer2=ee.Reducer.max().combine(\n",
    "                        reducer2=ee.Reducer.stdDev(),\n",
    "                        sharedInputs=True\n",
    "                    ),\n",
    "                    sharedInputs=True\n",
    "                ),\n",
    "                sharedInputs=True\n",
    "            ),\n",
    "            geometry=ee_polygon,\n",
    "            scale=10,\n",
    "            maxPixels=1e19\n",
    "        )\n",
    "        \n",
    "        # Add the statistics to the dictionary\n",
    "        statistics_dict[index] = {\n",
    "          # 'min': stats.get(f'{index}_min').getInfo(),\n",
    "          'median': stats.get(f'{index}_median').getInfo(),\n",
    "          # 'max': stats.get(f'{index}_max').getInfo(),\n",
    "          'stdv': stats.get(f'{index}_stdDev').getInfo()\n",
    "        }\n",
    "        \n",
    "    return statistics_dict\n",
    "\n",
    "# Function to convert the geometry to an ee.polygon and transform the projection\n",
    "def geometry_to_ee_polygon(geometry):\n",
    "        # Transform the geometry to WGS84 projection\n",
    "        source_crs = \"EPSG:2193\"\n",
    "        target_crs = \"EPSG:4326\"\n",
    "        transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "    \n",
    "        # Reproject the polygon to the target CRS\n",
    "        try:\n",
    "            # Check if the geometery is a polygon or a multipolygon type\n",
    "            if isinstance(geometry, MultiPolygon):\n",
    "                geometry_list = []\n",
    "                for polygon in geometry.geoms:\n",
    "                    # Reproject the polygon to the target CRS\n",
    "                    transformed_coordinates = [transformer.transform(x, y) for x, y in polygon.exterior.coords]\n",
    "                    transformed_geometry = Polygon(transformed_coordinates)\n",
    "                    \n",
    "                    # Create an ee.Geometry.Polygon and add it to a list\n",
    "                    ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "                    geometry_list += [ee_polygon]\n",
    "                    \n",
    "                    # Create a ee.Geometry.MultiPolygon using the list of ee.Geometry objects\n",
    "                    multi_polygon = ee.Geometry.MultiPolygon(geometry_list)\n",
    "                    # Dissolve the ee.Geometry.MultiPolygon to create a single ee.Geometry.Polygon\n",
    "                    ee_polygon = multi_polygon.dissolve()\n",
    "            \n",
    "            elif isinstance(geometry, Polygon):\n",
    "                # Reproject the polygon to the target CRS\n",
    "                transformed_coordinates = [transformer.transform(x, y) for x, y in geometry.exterior.coords]\n",
    "                transformed_geometry = Polygon(transformed_coordinates)\n",
    "                ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "        \n",
    "            else:\n",
    "                raise ValueError(\"Invalid geometry type. Must be Polygon or MultiPolygon.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Invalid geometry at index: {e}\")\n",
    "        \n",
    "        return ee_polygon\n",
    "\n",
    "# Function to process each sampling site and its dates\n",
    "def process_sampling_site(site):\n",
    "    # Get the sampling site id\n",
    "    seg_id = site['nzsegment']\n",
    "    print(\"We are collecting data for sampling site\", seg_id)\n",
    "\n",
    "    # Convert the geometry to an ee.polygon\n",
    "    ee_polygon = geometry_to_ee_polygon(site['geometry'])\n",
    "\n",
    "    # Define list of the calculated indices\n",
    "    indices = ['MNDWI', 'NDVI', 'NIRv', 'NSMI', 'BSI', 'EVI', 'SAVI', 'NDMI', 'NBR', 'CI', 'LAI', 'FAPAR']\n",
    "\n",
    "    # Get the sampling dates for the current sampling site\n",
    "    sampeling_dates = filtered_stack_frame['myDate'][filtered_stack_frame['nzsegment'] == seg_id].unique()\n",
    "    \n",
    "    dates_dict = {}\n",
    "    fixed_date = datetime.strptime(\"2017-12-03\", \"%Y-%m-%d\").date()\n",
    "    for date in sampeling_dates:\n",
    "        if date > fixed_date:\n",
    "            try:\n",
    "                START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "                END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "                print('.....Date in progress:', START_DATE, END_DATE)\n",
    "                \n",
    "                # Create a cloud free sentinel-2 image for a specific region and time\n",
    "                cld_free_image = cld_free_sl2(START_DATE, END_DATE, ee_polygon)\n",
    "                \n",
    "                # Calculated all RS indices and stack them into an EE Image.\n",
    "                indices_image = calculate_indices(cld_free_image, indices, ee_polygon)\n",
    "\n",
    "                # Calculate zonal stats\n",
    "                stats_date_indices_dic = calculate_stats(indices_image, indices, ee_polygon)\n",
    "       \n",
    "                # Define the output file path\n",
    "                stats_date_indices_dir = f'../Data/RS/RS_Indices_{seg_id}_{END_DATE}.p'\n",
    "                \n",
    "                # Save the dictionary as a pickle file\n",
    "                with open(stats_date_indices_dir, 'wb') as f:\n",
    "                    pickle.dump(stats_date_indices_dic, f)\n",
    "\n",
    "                # Add the stats result to the dictonary\n",
    "                dates_dict[END_DATE]  = stats_date_indices_dic\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Something went wrong: {e}\")\n",
    "                continue\n",
    "        \n",
    "    return dates_dict\n",
    "\n",
    "# The function to process each sampling site and its dates\n",
    "def process_sampling_site_wrapper(site):\n",
    "    try:\n",
    "        return process_sampling_site(site)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing site {site['nzsegment']}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc41092-1472-45a7-8343-cb65a370481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from datetime import date, timedelta, datetime\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "\n",
    "# Read the pickle data\n",
    "in_f_dir = '../Data/WQ_Data_Nov2021.p'\n",
    "with open(in_f_dir, 'rb') as f:\n",
    "    dat_dic = pickle.load(f)\n",
    "\n",
    "# Prepare the sampling dataset\n",
    "stack_frame = dat_dic['StackFrame']\n",
    "metadata = dat_dic['metadataWQ'].drop_duplicates()\n",
    "\n",
    "# Filter the sampling dataset to contain only 2019 and later\n",
    "filtered_stack_frame = stack_frame[stack_frame['Year'] > 2018]\n",
    "filtered_stack_frame = filtered_stack_frame.reset_index(drop=True)\n",
    "filtered_metadata = metadata[metadata['nzsegment'].isin(filtered_stack_frame['nzsegment'])]\n",
    "filtered_metadata = filtered_metadata.reset_index(drop=True)\n",
    "# filtered_metadata = filtered_metadata.head(2)\n",
    "\n",
    "# Initialize the empty dictionary for RS Indices\n",
    "rs_indices = {}\n",
    "\n",
    "# Process each sampling site using ThreadPoolExecutor for parallel processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_sampling_site_wrapper, site) for i, site in filtered_metadata.iterrows()]\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "        site_result = future.result()\n",
    "        if site_result is not None:\n",
    "            seg_id = filtered_metadata.loc[i, 'nzsegment']\n",
    "            rs_indices[seg_id] = site_result\n",
    "    rs_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d00108-f8c0-4fbb-99fa-90b1f1eecb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "    \n",
    "# Read the pickle data\n",
    "in_f_dir = '../Data/RS/RS_Indices_2032082.0_2020-12-07.p'\n",
    "with open(in_f_dir, 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641aa49c-886d-48e9-bfa4-af624cd7354b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Double check if any of the sampling sites and dates has been left out and try to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd9760-9927-44ad-8372-ccb8482d3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Function to extract ID and date from the filename using regular expression\n",
    "def extract_id_and_date_from_filename(filename):\n",
    "    pattern = r'RS_Indices_(\\d+\\.\\d+)_(\\d{4}-\\d{2}-\\d{2})\\.p'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Directory path where your files are located\n",
    "folder_path = r\"C:\\Users\\SaeedR\\Desktop\\Saeed\\App\\Data\\RS\"\n",
    "\n",
    "# Dictionary to store indexed IDs and their corresponding dates\n",
    "stored_ids_with_dates = {}\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a .p file\n",
    "    if filename.endswith(\".p\"):\n",
    "        # Extract ID and date from the filename\n",
    "        id_part, date_part = extract_id_and_date_from_filename(filename)\n",
    "        if id_part and date_part:\n",
    "            # Add the ID and date to the indexed dictionary\n",
    "            if id_part not in stored_ids_with_dates:\n",
    "                stored_ids_with_dates[id_part] = {\"Dates\": []}\n",
    "            stored_ids_with_dates[id_part][\"Dates\"].append(date_part)\n",
    "\n",
    "# # Print the indexed dictionary containing IDs and their corresponding dates\n",
    "# for index, (id_part, data) in enumerate(stored_ids_with_dates.items(), start=1):\n",
    "#     print(f\"ID{index}: {data}\")\n",
    "# stored_ids_with_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ad599-4367-4a62-a247-9ecb6dc2388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pyproj\n",
    "\n",
    "try:\n",
    "    import ee\n",
    "    print('The required modules are installed')\n",
    "except ModuleNotFoundError:\n",
    "    print('The required modules are NOT installed')\n",
    "\n",
    "    # Install the required modules\n",
    "    python = sys.executable\n",
    "    subprocess.check_call(\n",
    "        [python, '-m', 'pip', 'install', 'earthengine-api'],\n",
    "        stdout=subprocess.DEVNULL\n",
    "    )\n",
    "\n",
    "## Initialize the library.\n",
    "ee.Initialize()\n",
    "\n",
    "# Function to perform zonal stats\n",
    "def calculate_stats(image, indices, ee_polygon):    \n",
    "    # Create an empty dictionary\n",
    "    statistics_dict = {}\n",
    "    \n",
    "    # Calculate statistics for each index\n",
    "    for index in indices:\n",
    "        # Calculate multiple statistics using reduceRegion\n",
    "        stats = image.reduceRegion(\n",
    "            reducer=ee.Reducer.min().combine(\n",
    "                reducer2=ee.Reducer.median().combine(\n",
    "                    reducer2=ee.Reducer.max().combine(\n",
    "                        reducer2=ee.Reducer.stdDev(),\n",
    "                        sharedInputs=True\n",
    "                    ),\n",
    "                    sharedInputs=True\n",
    "                ),\n",
    "                sharedInputs=True\n",
    "            ),\n",
    "            geometry=ee_polygon,\n",
    "            scale=10,\n",
    "            maxPixels=1e19\n",
    "        )\n",
    "        \n",
    "        # Add the statistics to the dictionary\n",
    "        statistics_dict[index] = {\n",
    "          # 'min': stats.get(f'{index}_min').getInfo(),\n",
    "          'median': stats.get(f'{index}_median').getInfo(),\n",
    "          # 'max': stats.get(f'{index}_max').getInfo(),\n",
    "          'stdv': stats.get(f'{index}_stdDev').getInfo()\n",
    "        }\n",
    "        \n",
    "    return statistics_dict\n",
    "\n",
    "# Function to convert the geometry to an ee.polygon and transform the projection\n",
    "def geometry_to_ee_polygon(geometry):\n",
    "        # Transform the geometry to WGS84 projection\n",
    "        source_crs = \"EPSG:2193\"\n",
    "        target_crs = \"EPSG:4326\"\n",
    "        transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "    \n",
    "        # Reproject the polygon to the target CRS\n",
    "        try:\n",
    "            # Check if the geometery is a polygon or a multipolygon type\n",
    "            if isinstance(geometry, MultiPolygon):\n",
    "                geometry_list = []\n",
    "                for polygon in geometry.geoms:\n",
    "                    # Reproject the polygon to the target CRS\n",
    "                    transformed_coordinates = [transformer.transform(x, y) for x, y in polygon.exterior.coords]\n",
    "                    transformed_geometry = Polygon(transformed_coordinates)\n",
    "                    \n",
    "                    # Create an ee.Geometry.Polygon and add it to a list\n",
    "                    ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "                    geometry_list += [ee_polygon]\n",
    "                    \n",
    "                    # Create a ee.Geometry.MultiPolygon using the list of ee.Geometry objects\n",
    "                    multi_polygon = ee.Geometry.MultiPolygon(geometry_list)\n",
    "                    # Dissolve the ee.Geometry.MultiPolygon to create a single ee.Geometry.Polygon\n",
    "                    ee_polygon = multi_polygon.dissolve()\n",
    "            \n",
    "            elif isinstance(geometry, Polygon):\n",
    "                # Reproject the polygon to the target CRS\n",
    "                transformed_coordinates = [transformer.transform(x, y) for x, y in geometry.exterior.coords]\n",
    "                transformed_geometry = Polygon(transformed_coordinates)\n",
    "                ee_polygon = ee.Geometry.Polygon(transformed_coordinates)\n",
    "        \n",
    "            else:\n",
    "                raise ValueError(\"Invalid geometry type. Must be Polygon or MultiPolygon.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Invalid geometry at index: {e}\")\n",
    "        \n",
    "        return ee_polygon\n",
    "\n",
    "# Function to process each sampling site and its dates\n",
    "def process_sampling_site(site):\n",
    "    # Get the sampling site id\n",
    "    seg_id = site['nzsegment']\n",
    "    # print(\"We are collecting data for sampling site\", seg_id)\n",
    "\n",
    "    # Convert the geometry to an ee.polygon\n",
    "    ee_polygon = geometry_to_ee_polygon(site['geometry'])\n",
    "\n",
    "    # Define list of the calculated indices\n",
    "    indices = ['MNDWI', 'NDVI', 'NIRv', 'NSMI', 'BSI', 'EVI', 'SAVI', 'NDMI', 'NBR', 'CI', 'LAI', 'FAPAR']\n",
    "\n",
    "    # Get the sampling dates for the current sampling site\n",
    "    sampeling_dates = filtered_stack_frame['myDate'][filtered_stack_frame['nzsegment'] == seg_id].unique()\n",
    "    \n",
    "    for date in sampeling_dates:\n",
    "        START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "        END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Check if the ID exists in the dictionary\n",
    "        if str(seg_id) in stored_ids_with_dates:\n",
    "            # Get the dates list for the given ID\n",
    "            dates_list = stored_ids_with_dates[str(seg_id)][\"Dates\"]\n",
    "            # Check if the date exists in the dates list\n",
    "            if END_DATE not in dates_list:\n",
    "                try:\n",
    "                    START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "                    END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "                    print('.....Date in progress:', START_DATE, END_DATE)\n",
    "                    \n",
    "                    # Create a cloud free sentinel-2 image for a specific region and time\n",
    "                    cld_free_image = cld_free_sl2(START_DATE, END_DATE, ee_polygon)\n",
    "                    \n",
    "                    # Calculated all RS indices and stack them into an EE Image.\n",
    "                    indices_image = calculate_indices(cld_free_image, indices, ee_polygon)\n",
    "    \n",
    "                    # Calculate zonal stats\n",
    "                    stats_date_indices_dic = calculate_stats(indices_image, indices, ee_polygon)\n",
    "           \n",
    "                    # Define the output file path\n",
    "                    stats_date_indices_dir = f'../Data/RS/RS_Indices_{seg_id}_{END_DATE}.p'\n",
    "                    \n",
    "                    # Save the dictionary as a pickle file\n",
    "                    with open(stats_date_indices_dir, 'wb') as f:\n",
    "                        pickle.dump(stats_date_indices_dic, f)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Something went wrong: {e}\")\n",
    "                    continue\n",
    "\n",
    "        else:\n",
    "            print(f\"{seg_id} does not exist.\")\n",
    "            try:\n",
    "                START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "                END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "                print('.....Date in progress:', START_DATE, END_DATE)\n",
    "                \n",
    "                # Create a cloud free sentinel-2 image for a specific region and time\n",
    "                cld_free_image = cld_free_sl2(START_DATE, END_DATE, ee_polygon)\n",
    "                \n",
    "                # Calculated all RS indices and stack them into an EE Image.\n",
    "                indices_image = calculate_indices(cld_free_image, indices, ee_polygon)\n",
    "\n",
    "                # Calculate zonal stats\n",
    "                stats_date_indices_dic = calculate_stats(indices_image, indices, ee_polygon)\n",
    "       \n",
    "                # Define the output file path\n",
    "                stats_date_indices_dir = f'../Data/RS/RS_Indices_{seg_id}_{END_DATE}.p'\n",
    "                \n",
    "                # Save the dictionary as a pickle file\n",
    "                with open(stats_date_indices_dir, 'wb') as f:\n",
    "                    pickle.dump(stats_date_indices_dic, f)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Something went wrong: {e}\")\n",
    "                continue\n",
    "        \n",
    "    return []\n",
    "\n",
    "# The function to process each sampling site and its dates\n",
    "def process_sampling_site_wrapper(site):\n",
    "    try:\n",
    "        return process_sampling_site(site)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing site {site['nzsegment']}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bf880-7b05-4600-849f-c5dd6fe06166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from datetime import date, timedelta, datetime\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "\n",
    "# # Read the pickle data\n",
    "# in_f_dir = '../Data/WQ_Data_Nov2021.p'\n",
    "# with open(in_f_dir, 'rb') as f:\n",
    "#     dat_dic = pickle.load(f)\n",
    "\n",
    "# # Prepare the sampling dataset\n",
    "# stack_frame = dat_dic['StackFrame']\n",
    "# metadata = dat_dic['metadataWQ'].drop_duplicates()\n",
    "\n",
    "# # Filter the sampling dataset to contain only 2019 and later\n",
    "# filtered_stack_frame = stack_frame[stack_frame['Year'] > 2018]\n",
    "# filtered_stack_frame = filtered_stack_frame.reset_index(drop=True)\n",
    "# filtered_metadata = metadata[metadata['nzsegment'].isin(filtered_stack_frame['nzsegment'])]\n",
    "# filtered_metadata = filtered_metadata.reset_index(drop=True)\n",
    "# # filtered_metadata = filtered_metadata.head(2)\n",
    "\n",
    "# Initialize the empty dictionary for RS Indices\n",
    "rs_indices = {}\n",
    "\n",
    "# # Process each sampling site using the function\n",
    "# for i, site in filtered_metadata.iloc[0:5].iterrows():\n",
    "#     process_sampling_site(site)\n",
    "\n",
    "# Process each sampling site using ThreadPoolExecutor for parallel processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_sampling_site_wrapper, site) for i, site in filtered_metadata.iterrows()]\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "        site_result = future.result()\n",
    "        if site_result is not None:\n",
    "            seg_id = filtered_metadata.loc[i, 'nzsegment']\n",
    "            rs_indices[seg_id] = site_result\n",
    "    rs_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129ad00-e0e9-4148-8531-3b05e1dc68f6",
   "metadata": {},
   "source": [
    "#### Read through the downloaded satelite idices files and store them in one pickel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff43f3ee-7be6-4a3a-b84b-0226a24d73bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been combined and saved to RS_Indices.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Path to the folder containing the files\n",
    "folder_path = '../Data/RS'\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize a dictionary to store the combined data\n",
    "combined_data = {}\n",
    "\n",
    "# Loop through each file and extract the ID and date from its name\n",
    "for file_name in file_list:\n",
    "    # Split the file name by '_' to get individual parts\n",
    "    parts = file_name.split('_')\n",
    "    if len(parts) == 4 and parts[0] == 'RS' and parts[3].endswith('.p'):\n",
    "        # Extract the ID and date from the file name\n",
    "        ID = float(parts[2])\n",
    "        date = parts[3].rstrip('.p')\n",
    "\n",
    "        # Read the nested dictionary data from the file\n",
    "        with open(os.path.join(folder_path, file_name), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # Create a nested dictionary for the date data\n",
    "        date_data = {date: data}\n",
    "\n",
    "        # Add the date data to the combined_data dictionary using the ID as the key\n",
    "        if ID not in combined_data:\n",
    "            combined_data[ID] = date_data\n",
    "        else:\n",
    "            combined_data[ID].update(date_data)\n",
    "\n",
    "# Write the combined_data dictionary to a new file\n",
    "output_file = 'RS_Indices.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(combined_data, f)\n",
    "\n",
    "print(\"All data has been combined and saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3279a4c1-6aac-45f4-88c4-9498fe3b99e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MNDWI': {'median': -0.23048481042765315, 'stdv': 0.11830447240975914},\n",
       " 'NDVI': {'median': 0.763530117268573, 'stdv': 0.13798516389184692},\n",
       " 'NIRv': {'median': 1967.720253165391, 'stdv': 1108.1637028557248},\n",
       " 'NSMI': {'median': 0.4649228650557418, 'stdv': 0.13964703365675774},\n",
       " 'BSI': {'median': -0.7324171758438287, 'stdv': 0.06980831935221155},\n",
       " 'EVI': {'median': 4.724472945229572, 'stdv': 81.96784369531181},\n",
       " 'SAVI': {'median': 1.144301946960723, 'stdv': 0.2069559350562196},\n",
       " 'NDMI': {'median': 0.4649228650557418, 'stdv': 0.13964703365675774},\n",
       " 'NBR': {'median': 0.7071415739227616, 'stdv': 0.13846370837561323},\n",
       " 'CI': {'median': -0.41791865445581633, 'stdv': 0.1368447635644697},\n",
       " 'LAI': {'median': 3.740117611514754e+153, 'stdv': 'NaN'},\n",
       " 'FAPAR': {'median': 0.9648519727161867, 'stdv': 0.13872007714264617}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "    \n",
    "# Read the pickle data\n",
    "in_f_dir = '../Data/RS/RS_Indices_2032082.0_2020-12-07.p'\n",
    "with open(in_f_dir, 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7800e-aa29-4020-b1d5-b45707141d23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Iterate over each observation time and collect the satelite indices for all sampling sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d57952-e3a7-4eb3-b74a-14c3fe214564",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Calculate a list of indices for a specific site and return the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102742f-d996-4a3b-96fb-2447749fbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Function to calculate a list of indices for a specific site and return the image\n",
    "def calculate_indices(img, indices, ee_polygon):\n",
    "    # Initialize an empty ee.Image to store the calculated indices\n",
    "    calc_indices = ee.Image()\n",
    "    \n",
    "    # Iterate over the list of indices and calculate each requested index in the list\n",
    "    for index in indices:\n",
    "        if 'MNDWI' in indices: # Modified Normalized Difference Water Index     \n",
    "            mndwi = img.normalizedDifference(['B3','B11']).rename(\"MNDWI\") \n",
    "            calc_indices = calc_indices.addBands(mndwi)\n",
    "            \n",
    "        elif 'NDVI' in indices: # Normalized Difference Vegetation Index\n",
    "            ndvi = img.normalizedDifference(['B8','B4']).rename(\"NDVI\")\n",
    "            calc_indices = calc_indices.addBands(ndvi)\n",
    "            \n",
    "        elif 'NIRv': # Another vegetation index\n",
    "            ndvi = img.normalizedDifference(['B8','B4']).rename(\"NDVI\") \n",
    "            nir = img.select('B8')\n",
    "            nirv = ndvi.multiply(nir).rename(\"NIRv\")\n",
    "            calc_indices = calc_indices.addBands(nirv)\n",
    "            \n",
    "        elif 'NSMI' in indices: # Normalized Soil Index\n",
    "            nsmi = img.normalizedDifference(['B8','B11']).rename(\"NSMI\")\n",
    "            calc_indices = calc_indices.addBands(nsmi)\n",
    "            \n",
    "        elif 'BSI' in indices: # Bare Soil Index\n",
    "            # Calculate the Bare Soil Index (BSI)\n",
    "            bsi = img.normalizedDifference(['B3', 'B11'])\\\n",
    "                    .subtract(img.select('B8')).divide(img.select('B11')\\\n",
    "                                                       .add(img.select('B8')))\n",
    "            # Rename the BSI band\n",
    "            bsi = bsi.rename(\"BSI\")\n",
    "            calc_indices = calc_indices.addBands(bsi)\n",
    "            \n",
    "        elif 'EVI' in indices: # Enhanced Vegetation Index\n",
    "            evi = img.expression('2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',\\\n",
    "                                 {'NIR': img.select('B8'), 'RED': img.select('B4'),\\\n",
    "                                  'BLUE': img.select('B2')}).rename(\"EVI\")\n",
    "            calc_indices = calc_indices.addBands(evi)\n",
    "        \n",
    "        elif 'SAVI' in indices: # Soil Adjusted Vegetation Index\n",
    "            savi = img.expression('(1 + 0.5) * (NIR - RED) / (NIR + RED + 0.5)',\\\n",
    "                                  {'NIR': img.select('B8'), 'RED': img.select('B4')})\\\n",
    "                                   .rename(\"SAVI\")\n",
    "            calc_indices = calc_indices.addBands(savi)\n",
    "            \n",
    "        elif 'NDMI' in indices: # Normalized Difference Moisture Index\n",
    "            ndmi = img.normalizedDifference(['B8', 'B11']).rename(\"NDMI\")\n",
    "            calc_indices = calc_indices.addBands(ndmi)\n",
    "            \n",
    "        elif 'NBR' in indices: # Normalized Burn Ratio\n",
    "            nbr = img.normalizedDifference(['B8', 'B12']).rename(\"NBR\")\n",
    "            calc_indices = calc_indices.addBands(nbr)\n",
    "            \n",
    "        elif 'CI' in indices: # Chlorophyll Index\n",
    "            ci = img.select('B4').divide(img.select('B3')).subtract(1).rename(\"CI\")\n",
    "            calc_indices = calc_indices.addBands(ci)\n",
    "            \n",
    "        elif 'LAI' in indices: # Leaf Area Index\n",
    "            lai = img.expression('3.618 * exp(-0.488 * (RED - GREEN))',\\\n",
    "                                 {'RED': img.select('B4'), 'GREEN': img.select('B3')}).rename(\"LAI\")\n",
    "            calc_indices = calc_indices.addBands(lai)\n",
    "            \n",
    "        elif 'FAPAR' in indices: # Fraction of Absorbed Photosynthetically Active Radiation\n",
    "            fapar = img.expression('(NIR - RED) / (0.88 * NIR + 0.12 * RED)',\\\n",
    "                                   {'NIR': img.select('B8'), 'RED': img.select('B4')}).rename(\"FAPAR\")\n",
    "            calc_indices = calc_indices.addBands(fapar)\n",
    "\n",
    "    # # Copy the properties from img to calc_indices to ensure the same metadata\n",
    "    # calc_indices = calc_indices.copyProperties(img)\n",
    "    \n",
    "    # Clip the index image to the extent of the current sampling site catchment\n",
    "    calc_indices = calc_indices.clip(ee_polygon)\n",
    "\n",
    "    return calc_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e2c80-e5eb-4632-9b50-b239102851a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Claculate Zonal Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9805ad-8ab1-4b75-90b5-52ce2e163a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "def calculate_stats(image, feature):\n",
    "    # Get the geometry of the feature\n",
    "    geom = feature.geometry()\n",
    "    \n",
    "    # Reduce the image within the polygon to get median and stdDev\n",
    "    reducer = ee.Reducer.median().combine(ee.Reducer.stdDev(), '', True)\n",
    "    stats = image.reduceRegion(reducer, geom, 30)\n",
    "    \n",
    "    # Extract the median and stdDev values from the computed dictionary\n",
    "    median_vals = [stats.get(f'{band}_median') for band in image.bandNames().getInfo()]\n",
    "    std_dev_vals = [stats.get(f'{band}_stdDev') for band in image.bandNames().getInfo()]\n",
    "    \n",
    "    # Create a dictionary containing band names and their corresponding median and stdDev values\n",
    "    band_stats = dict(zip(image.bandNames().getInfo(), zip(median_vals, std_dev_vals)))\n",
    "    \n",
    "    # Return the dictionary along with the feature's properties\n",
    "    return feature.set(band_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e716809-42ac-4834-bc53-aa65d579721b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631171dd-7bd4-49a2-9a8c-768ad1c0c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading required libraries ....\")\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from datetime import date, timedelta, datetime\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "try:\n",
    "  import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "  import pickle\n",
    "\n",
    "# Read the pickle data\n",
    "in_f_dir = '../Data/WQ_Data_Nov2021.p'\n",
    "with open(in_f_dir, 'rb') as f:\n",
    "    dat_dic = pickle.load(f)\n",
    "print(\"All required libraries were successfully added\")\n",
    "\n",
    "print(\"Loading the sampling dataset ....\")\n",
    "# Prepare the sampling dataset\n",
    "stack_frame = dat_dic['StackFrame']\n",
    "metadata = dat_dic['metadataWQ'].drop_duplicates()\n",
    "print(\"The sampeling dataset was successfully loaded\")\n",
    "\n",
    "print(\"Preparing the samppeling dataset\")\n",
    "# Filter the sampling dataset to contain only 2019 and later\n",
    "filtered_stack_frame = stack_frame[stack_frame['Year'] > 2018]\n",
    "filtered_stack_frame = filtered_stack_frame.reset_index(drop=True)\n",
    "filtered_metadata = metadata[metadata['nzsegment'].isin(filtered_stack_frame['nzsegment'])]\n",
    "filtered_metadata = filtered_metadata.reset_index(drop=True)\n",
    "\n",
    "print(\"Loading the sampling site polygons ...\")\n",
    "# Load the shapefile using geopandas\n",
    "shapefile_path = '../Data/SamplingSites_CZ_Topo.shp'\n",
    "# shapefile = gpd.read_file(shapefile_path)\n",
    "print(\"The sampling site polygons was successfully loaded\")\n",
    "\n",
    "print(\"Converting the sampling site polygons shapefile into ee.FeatureCollection ...\")\n",
    "# Convert Polygon Shapefile into ee.FeatureCollection \n",
    "feature_collection = shapefile_2_ee_collection(shapefile)\n",
    "print(\"The sampling site polygons shapefile was successfully converted into ee.FeatureCollection\")\n",
    "\n",
    "# Define the bounding box for New Zealand [west, south, east, north]\n",
    "new_zealand_geometry = ee.Geometry.Rectangle([165.0, -47.5, 179.0, -34.0])\n",
    "\n",
    "print(\"Getting the sampling times ...\")\n",
    "# Get the sampling dates\n",
    "sampeling_dates = filtered_stack_frame['myDate'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4891b4-bbb5-4e1e-98eb-07a14b43f878",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1146e7-53c0-4ade-980f-82b9629a86f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Single computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffeb91-8cb9-438e-b4eb-f4e292be814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define list of the calculated indices\n",
    "# indices = ['MNDWI', 'NDVI', 'NIRv', 'NSMI', 'BSI', 'EVI', 'SAVI', 'NDMI', 'NBR', 'CI', 'LAI', 'FAPAR']\n",
    "\n",
    "# date_dict = {}\n",
    "# for date in sampeling_dates:\n",
    "#     try:\n",
    "#         START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "#         END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "#         print(\"The Image for Date :\", START_DATE, END_DATE, \"is getting collected from Google Earth Engine\")\n",
    "        \n",
    "#         # Create a cloud free sentinel-2 image for a specific region and time\n",
    "#         cld_free_image =  cld_free_sl2(START_DATE, END_DATE, new_zealand_geometry)\n",
    "        \n",
    "#         # Calculate a list of indices for a specific site and return the image\n",
    "#         indices_img = calculate_indices(cld_free_image, indices, new_zealand_geometry)\n",
    "        \n",
    "        \n",
    "#         # Calculate the statistics for each polygon in the feature_collection\n",
    "#         stats_by_polygon = feature_collection.map(lambda feature: calculate_stats(indices_img, feature))\n",
    "        \n",
    "#         # Print the result\n",
    "#         print(stats_by_polygon.getInfo())\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Something went wrong: {e}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d706cb-9af4-427b-808f-687bdf784ffe",
   "metadata": {},
   "source": [
    "#### Batch Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14353228-4a3f-4de5-9409-0be016337f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define list of the calculated indices\n",
    "# indices = ['MNDWI', 'NDVI', 'NIRv', 'NSMI', 'BSI', 'EVI', 'SAVI', 'NDMI', 'NBR', 'CI', 'LAI', 'FAPAR']\n",
    "\n",
    "# # Define the batch size (number of features per batch)\n",
    "# batch_size = 50\n",
    "# # Split the feature_collection into smaller batches\n",
    "# feature_batches = []\n",
    "# num_features = feature_collection.size().getInfo()\n",
    "# for i in range(0, num_features, batch_size):\n",
    "#     batch = feature_collection.toList(batch_size, i)\n",
    "#     feature_batches.append(ee.FeatureCollection(batch))\n",
    "\n",
    "# print(\"Calculating Zonal Stats for each date ...\")\n",
    "# date_dict = {}\n",
    "# # Iterate over the sampling dates and calculate the RS indices for all polygons in each date\n",
    "# for date in sampeling_dates:\n",
    "#     START_DATE = (date - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "#     END_DATE = date.strftime(\"%Y-%m-%d\")\n",
    "#     print(\"The Image for Date :\", START_DATE, END_DATE, \"is getting collected from Google Earth Engine\")\n",
    "\n",
    "#     # Process each batch\n",
    "#     for batch in feature_batches:\n",
    "#         try:\n",
    "#             # Create a cloud free sentinel-2 image for a specific region and time\n",
    "#             cld_free_image = cld_free_sl2(START_DATE, END_DATE, new_zealand_geometry)\n",
    "            \n",
    "#             # Calculate a list of indices for a specific site and return the image\n",
    "#             indices_img = calculate_indices(cld_free_image, indices, new_zealand_geometry)\n",
    "            \n",
    "#             # Calculate the statistics for each polygon in the batch\n",
    "#             stats_by_polygon = batch.map(lambda feature: calculate_stats(indices_img, feature))\n",
    "            \n",
    "#             # Print the result for this batch\n",
    "#             print(stats_by_polygon.getInfo())\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Something went wrong: {e}\")\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ab1ee-aeb5-4e18-83be-aa1a643510f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
